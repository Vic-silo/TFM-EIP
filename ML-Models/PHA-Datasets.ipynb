{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "894b7838-b63b-4c1d-ac15-2893c49ce9c7",
   "metadata": {},
   "source": [
    "# ANALISIS DE RIESGOS: Crear dataframe\n",
    "\n",
    "En este notebook se crea un dataframe que englobe todos los existentes para los diferentes lustros.\n",
    "Se crea un fichero .csv padre del cual parta los diferentes análisis a realizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1429a5e0-928f-48c3-8d98-1efe211dfe5b",
   "metadata": {},
   "source": [
    "### PREPARACION DE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a77d9ea-a599-49aa-90d4-8ff98f2ee960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import pandas as pd\n",
    "import csv\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e92421-48b7-45dc-92c9-07cc7396b33e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fuente\n",
    "\n",
    "https://av-info.faa.gov/dd_sublevel.asp?Folder=%5CAID\n",
    "\n",
    "Todos los datos obtenidos, se realiza mediante el acceso a la pagina web de la fuente. De este modo, en el momento de obtener los datos de estudio, se asegura que estos estén actualizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07a48cac-fc1d-4f67-b013-37ec10566149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de los datos a analizar: Leyenda\n",
    "url_legend = 'https://av-info.faa.gov/data/AID/Afilelayout.txt'\n",
    "\n",
    "# URL de los datos a analizar: Lustros con datos de estudio de 1975 a actualidad\n",
    "# Obtener ficheros que se desea obtener los datos\n",
    "lust = [year for year in range(1975, 2025, 5)]\n",
    "AID_files = [f'a{year}_{str(year+4)[-2:]}.txt' for year in lust]\n",
    "# El fichero de datos actual, sabemos que es de 2020 a 2025 actualemnte\n",
    "AID_files[-1:] = 'a2020_25.txt',\n",
    "\n",
    "with urlopen(\"https://av-info.faa.gov/dd_sublevel.asp?Folder=%5CAID\") as r:\n",
    "    bs = BeautifulSoup(r.read(), \"html.parser\")\n",
    "\n",
    "url_AID = []\n",
    "for link in bs.find_all(\"a\"):\n",
    "    # Buscar el texto del fichero en los ficheros creados previamente\n",
    "    # Se desea aquellos que estan delimitados por \\t, por ello buscar /tab/\n",
    "    if link.next_element in AID_files and '/tab/' in link.get(\"href\"):\n",
    "        url_AID.append(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce69e7e-9f5a-464d-ba53-86d3c899291d",
   "metadata": {},
   "source": [
    "##### Leyenda de datos\n",
    "\n",
    "Extraer del fichero de leyendas, la referencia a los nombres de columnas para poder validar posteriormente los datos de los datasets.<br>\n",
    "Se accede al contenido de los datos desde internet, de este modo, existe una referencia actualizada de los datos al obtener estos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26511b85-b93e-4946-b58c-a69a5648e6fe",
   "metadata": {},
   "source": [
    "Dar formato al .txt descargado para poder realizar su lectura posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30942a3c-2288-48ca-9c62-331feefc4b57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column_name</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c5</td>\n",
       "      <td>Unique control number used to relate to AID_MA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c1</td>\n",
       "      <td>Type of Event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c2</td>\n",
       "      <td>FAR part number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c3</td>\n",
       "      <td>Form on which the latest data was received.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c4</td>\n",
       "      <td>Agency conducting investigation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>c163</td>\n",
       "      <td>2nd Additional cause factor text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>c183</td>\n",
       "      <td>Supporting Factor Text                        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>c191</td>\n",
       "      <td>Supporting cause factor B text.              O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>c229</td>\n",
       "      <td>Date of Birth of PIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>c230</td>\n",
       "      <td>Date of birth of second Pilot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Column_name                                        Description\n",
       "0            c5  Unique control number used to relate to AID_MA...\n",
       "1            c1                                      Type of Event\n",
       "2            c2                                    FAR part number\n",
       "3            c3        Form on which the latest data was received.\n",
       "4            c4                   Agency conducting investigation.\n",
       "..          ...                                                ...\n",
       "174        c163                   2nd Additional cause factor text\n",
       "175        c183  Supporting Factor Text                        ...\n",
       "176        c191  Supporting cause factor B text.              O...\n",
       "177        c229                               Date of Birth of PIC\n",
       "178        c230                      Date of birth of second Pilot\n",
       "\n",
       "[179 rows x 2 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leemos el contenido de la leyenda\n",
    "with urlopen(url_legend) as content:\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    soup_lines = str(soup).split('\\r\\n')\n",
    "\n",
    "# Transformamos la respuesta en un diccionario con el nombre de la columna y la descripción\n",
    "legend_df = {\"Column_name\": [], \"Description\": []}\n",
    "# Recorrer las lineas con datos de lineas. Se salta las dos primeras y las tres ultimas\n",
    "# filas por no tener datos relevantes de la leyenda\n",
    "for line in soup_lines[2:-3]:\n",
    "    # Se extrae los 5 primeros caracteres para conocer el nombre de la columna\n",
    "    legend_df[\"Column_name\"].append(line[0:5].strip())\n",
    "    # Se extrae la descripcion, esta comienza en la posicion 53\n",
    "    legend_df[\"Description\"].append(line[53:].strip())\n",
    "    \n",
    "# Convertir el diccionario en pandas.Dataframe\n",
    "legend_df = pd.DataFrame.from_dict(data=legend_df)\n",
    "legend_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad6ff4a-4f14-4712-b165-39b852ed85f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Obtener dataframes\n",
    "\n",
    "Leer y validar todos los ficheros con datos para realizar PHA.<br>\n",
    "\n",
    "Para obtener los datos, ya que se trata de ficheros muy pesado, se realiza la descarga de los mismos y se trabaja el análisis para crear el dataset desde los ficheros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d4021dc-6a49-443b-b695-68cbf9f23229",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Datasets/PHA/'\n",
    "pha_file = 'PHA_Data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "29eab8e3-32d3-442b-9d53-ca60ed33ec27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] FILE_CREATED\ta1975_79.txt\n",
      "[+] FILE_CREATED\ta1980_84.txt\n",
      "[+] FILE_CREATED\ta1985_89.txt\n",
      "[+] FILE_CREATED\ta1990_94.txt\n",
      "[+] FILE_CREATED\ta1995_99.txt\n",
      "[+] FILE_CREATED\ta2000_04.txt\n",
      "[+] FILE_CREATED\ta2005_09.txt\n",
      "[+] FILE_CREATED\ta2010_14.txt\n",
      "[+] FILE_CREATED\ta2015_19.txt\n",
      "[+] FILE_CREATED\ta2020_25.txt\n"
     ]
    }
   ],
   "source": [
    "# Descarga de ficheros previamente encontrados con BeautifulSoup\n",
    "for url, file_name in zip(url_AID, AID_files):\n",
    "    # Se envía la petición HTTP Get para la obtención de los datos\n",
    "    data = requests.get(url)\n",
    "\n",
    "    # Guardamos el archivo de manera local\n",
    "    with open(data_path+file_name, 'wb')as file:\n",
    "        file.write(data.content)\n",
    "        print(f'[+] FILE_CREATED\\t{file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5776ed-eaa3-4e7d-92b1-b9ebb3ae10da",
   "metadata": {},
   "source": [
    "Con los ficheros descargados, se crea un dataframe que albergue todos los datos. Para ello, se validará que las columnas existentes en los sucesivos ficheros, existen en la leyenda previamente extraida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9790ba7b-ef9a-4aa3-9b82-2cbd18d6f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_df(df) -> None:\n",
    "    \"\"\"\n",
    "    Validar las columnas existentes en el dataframe creado\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        # Saltar la columna \"end_of_record\"\n",
    "        if column == 'end_of_record':\n",
    "            continue\n",
    "        # Acceder al registro de la leyenda y comprobar que hay resultados\n",
    "        if legend_df.loc[legend_df['Column_name'] == column].empty:\n",
    "            raise AssertionError(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e091953c-f434-4096-9a8f-87fbe0a70f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Success:  Datasets/PHA/a1975_79.txt\n",
      "[+] Success:  Datasets/PHA/a1980_84.txt\n",
      "[+] Success:  Datasets/PHA/a1985_89.txt\n",
      "[+] Success:  Datasets/PHA/a1990_94.txt\n",
      "[+] Success:  Datasets/PHA/a1995_99.txt\n",
      "[+] Success:  Datasets/PHA/a2000_04.txt\n",
      "[+] Success:  Datasets/PHA/a2005_09.txt\n",
      "[+] Success:  Datasets/PHA/a2010_14.txt\n",
      "[+] Success:  Datasets/PHA/a2015_19.txt\n",
      "[+] Success:  Datasets/PHA/a2020_25.txt\n"
     ]
    }
   ],
   "source": [
    "# Ruta de acceso a los ficheros descargados\n",
    "datasets = [data_path + file for file in AID_files]\n",
    "\n",
    "# Crear un dataframe que posteriormente ira albergando el resto de dataframes que se crean\n",
    "try:\n",
    "    # Crear un dataframe que posteriormente ira albergando el resto de dataframes que se crean\n",
    "    dataset = datasets[0]\n",
    "    doc = codecs.open(dataset, 'rU')\n",
    "    df = pd.read_csv(doc, sep='\\t', on_bad_lines='skip', low_memory=False)\n",
    "    \n",
    "    # Validar dataset\n",
    "    validate_df(df)\n",
    "        \n",
    "    print('[+] Success: ', dataset)\n",
    "\n",
    "    for dataset in datasets[1:]:\n",
    "        doc = codecs.open(dataset, 'rU')\n",
    "        aux_df = pd.read_csv(doc, sep='\\t', on_bad_lines='skip', low_memory=False)\n",
    "\n",
    "        # Validar dataset\n",
    "        validate_df(aux_df)            \n",
    "\n",
    "        # Añadir nuevos datos al dataframe\n",
    "        # Se añade los nuevos registros ordenados por columnas\n",
    "        df = pd.concat([df, aux_df],axis=0)\n",
    "        del aux_df\n",
    "        print('[+] Success: ', dataset)\n",
    "        \n",
    "except AssertionError as e:\n",
    "    print(f'[-] BAD_FILE\\t{dataset}\\tCOLUMN_ERROR\\t{e.__str__()}')\n",
    "    del df\n",
    "    \n",
    "    doc = codecs.open(dataset, 'rU')\n",
    "    error_df = pd.read_csv(doc, sep='\\t', on_bad_lines='skip', low_memory=False)\n",
    "    display(error_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d1eba-708d-46b8-9b33-815a3c0354ba",
   "metadata": {},
   "source": [
    "###### Nota:\n",
    "Durante la validación, el fichero \"a1990_94.txt\" tiene un error en la columna \"32\":\n",
    "[-] BAD_FILE\tDatasets/PHA/a1990_94.txt\tCOLUMN_ERROR\t32\n",
    "\n",
    "Tras realizar las comprobaciones pertinentes, se observa que se debe a un mal tipado en la fuente de los datos y esta columna debería ser \"c32\". Por lo tanto, se modifica el fichero y se vuelve a realizar las comprobaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdbeda2-a435-4d7c-a343-0e694bde346e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf2001-ac29-492b-81c8-175cb081ca19",
   "metadata": {},
   "source": [
    "Una vez validado el dataframe, debido al gran volumen de datos que existe, se guarda el fichero en formato .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "113618d2-88f8-4cbd-833f-ca27f33c76ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [82]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpha_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python3_8_5\\lib\\site-packages\\pandas\\util\\_decorators.py:207\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python3_8_5\\lib\\site-packages\\pandas\\core\\frame.py:2835\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   2749\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2750\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   2751\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2831\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   2832\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2833\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 2835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2836\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2843\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2844\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python3_8_5\\lib\\site-packages\\pandas\\io\\parquet.py:416\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    415\u001b[0m     partition_cols \u001b[38;5;241m=\u001b[39m [partition_cols]\n\u001b[1;32m--> 416\u001b[0m impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m    420\u001b[0m impl\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[0;32m    421\u001b[0m     df,\n\u001b[0;32m    422\u001b[0m     path_or_buf,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    428\u001b[0m )\n",
      "File \u001b[1;32mc:\\python3_8_5\\lib\\site-packages\\pandas\\io\\parquet.py:52\u001b[0m, in \u001b[0;36mget_engine\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     50\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m     )\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "df.to_parquet(data_path + pha_file, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
