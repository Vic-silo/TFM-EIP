{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "894b7838-b63b-4c1d-ac15-2893c49ce9c7",
   "metadata": {},
   "source": [
    "# ANALISIS DE RIESGOS: Crear dataframe\n",
    "\n",
    "En este notebook se crea un dataframe que englobe todos los existentes para los diferentes lustros.\n",
    "Se crea un fichero .csv padre del cual parta los diferentes análisis a realizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1429a5e0-928f-48c3-8d98-1efe211dfe5b",
   "metadata": {},
   "source": [
    "### PREPARACION DE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a77d9ea-a599-49aa-90d4-8ff98f2ee960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import pandas as pd\n",
    "import csv\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce69e7e-9f5a-464d-ba53-86d3c899291d",
   "metadata": {},
   "source": [
    "##### Leyenda de datos\n",
    "\n",
    "Extraer del fichero de leyendas, la referencia a los nombres de columnas para poder validar posteriormente los datos de los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b79fd2-8e48-4335-b36e-2efd8abb63e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "info_path = 'Datasets/PHA/info/Afilelayout.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26511b85-b93e-4946-b58c-a69a5648e6fe",
   "metadata": {},
   "source": [
    "Dar formato al .txt descargado para poder realizar su lectura posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30942a3c-2288-48ca-9c62-331feefc4b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Se bypasea este codigo ya que el fichero ya ha sido transformado\n",
    "if False:\n",
    "    # Leemos el fichero\n",
    "    with open(info_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Transformamos el fichero del modo que sabemos que los datos estan presentados\n",
    "    # en este para delimitarlo por comas\n",
    "    new_text = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Eliminar espacios iniciales y finales\n",
    "        line = line.strip()\n",
    "        word = ''\n",
    "        new_line = ''\n",
    "\n",
    "        # Crear palabras según esta esté dentro de la linea\n",
    "        for ind, ch in enumerate(line[:-1]):\n",
    "            next_ch = line[ind+1]\n",
    "\n",
    "            # Tras una letra existe diferentes espacios en blanco hasta la\n",
    "            # columna siguiente\n",
    "            if ' ' not in [ch, next_ch]:\n",
    "                word += ch\n",
    "\n",
    "            # Añadir ultima letra y espaciar con coma si no se trata de la \n",
    "            # descripción de la columna\n",
    "            if ind < len(line)-2:\n",
    "                if ch != ' ' and next_ch == ' ':\n",
    "                    word += ch\n",
    "                    \n",
    "                    if ch.isalpha() and line[ind+2].isalpha():\n",
    "                        word += ' '\n",
    "                        continue\n",
    "\n",
    "                    new_line += word + ','\n",
    "                    word = ''\n",
    "\n",
    "        # Añadir en la ultima palabra el salto de linea y agregar la linea al texto\n",
    "        word += next_ch\n",
    "        new_line += word + '\\n'\n",
    "        \n",
    "        new_text.append(new_line)\n",
    "\n",
    "\n",
    "    # Volver a crear el documento\n",
    "    with open(info_path, 'w') as f:\n",
    "        f.writelines(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044d5811-db43-4a05-a2a0-c7908aaa11fe",
   "metadata": {},
   "source": [
    "Crear diccionario para acceder a los datos de la leyenda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742b4262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Numero de registros:  180\n"
     ]
    }
   ],
   "source": [
    "# Convertimos la descripción de las columnas en un diccionario para que el acceso sea sencillo y rápido\n",
    "dic_legend = {}\n",
    "\n",
    "with open(info_path) as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader) # Descartamos cabecera\n",
    "    for row in reader:\n",
    "        dic_legend[row[0]] = row[1:5]\n",
    "        \n",
    "print('[+] Numero de registros: ', len(dic_legend))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad6ff4a-4f14-4712-b165-39b852ed85f5",
   "metadata": {},
   "source": [
    "##### Obtener dataframes\n",
    "\n",
    "Leer y validar todos los ficheros con datos para realizar PHA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9790ba7b-ef9a-4aa3-9b82-2cbd18d6f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_df(df) -> bool:\n",
    "    \"\"\"\n",
    "    Validar las columnas existentes en el dataframe creado\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        # Acceder al registro de la leyenda\n",
    "        if not dic_legend.get(column, None):\n",
    "            return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20bdbdc-29f3-4271-9946-1eb7598be882",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Datasets/PHA/'\n",
    "pha_file = 'PHA_Data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e091953c-f434-4096-9a8f-87fbe0a70f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Success:  Datasets/PHA/a1995_99.txt\n",
      "[+] Success:  Datasets/PHA/a2000_04.txt\n",
      "[+] Success:  Datasets/PHA/a2005_09.txt\n",
      "[+] Success:  Datasets/PHA/a2010_14.txt\n",
      "[+] Success:  Datasets/PHA/a2015_19.txt\n",
      "[+] Success:  Datasets/PHA/A2020_25.txt\n"
     ]
    }
   ],
   "source": [
    "# Crear lista con los nombres de los datasets existentes\n",
    "datasets = [data_path + f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "\n",
    "# Crear un dataframe que posteriormente ira albergando el resto de dataframes que se crean\n",
    "try:\n",
    "    unused_files = [data_path + pha_file, data_path + 'AcrftSer.txt', data_path + 'Airport.txt']\n",
    "    # Crear un dataframe que posteriormente ira albergando el resto de dataframes que se crean\n",
    "    dataset = datasets[0]\n",
    "    doc = codecs.open(dataset, 'rU')\n",
    "    df = pd.read_csv(doc, sep='\\t', on_bad_lines='skip', low_memory=False)\n",
    "    \n",
    "    # Validar dataset\n",
    "    if not validate_df(df):\n",
    "        raise AssertionError\n",
    "\n",
    "    for dataset in datasets[1:]:\n",
    "        # Los ficheros dataset tienen _ en su texto\n",
    "        if dataset not in unused_files:\n",
    "            doc = codecs.open(dataset, 'rU')\n",
    "            aux_df = pd.read_csv(doc, sep='\\t', on_bad_lines='skip', low_memory=False)\n",
    "            \n",
    "            # Validar dataset\n",
    "            if not validate_df(aux_df):\n",
    "                raise AssertionError\n",
    "            \n",
    "            # Añadir nuevos datos al dataframe\n",
    "            # Se añade los nuevos registros ordenados por columnas\n",
    "            df = pd.concat([df, aux_df],axis=0)\n",
    "            del aux_df\n",
    "            print('[+] Success: ', dataset)\n",
    "        \n",
    "except AssertionError as e:\n",
    "    print('[-] Fichero error: ', dataset)\n",
    "    print('[-] Error: ', e.__str__())\n",
    "    del df\n",
    "    \n",
    "    doc = codecs.open(dataset, 'rU')\n",
    "    error_df = pd.read_csv(doc, sep='\\t', on_bad_lines='skip', low_memory=False)\n",
    "    display(error_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf2001-ac29-492b-81c8-175cb081ca19",
   "metadata": {},
   "source": [
    "Una vez validado el dataframe, lo guardamos como .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "113618d2-88f8-4cbd-833f-ca27f33c76ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(data_path + pha_file, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
